import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
from urllib.parse import urljoin
import re


class DXGovCrawlerWithEmbedding:
    def __init__(self, output_dir="van_ban_downloads"):
        self.base_url = "https://dx.gov.vn"
        self.output_dir = output_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def get_total_pages(self):
        """L·∫•y t·ªïng s·ªë trang b·∫±ng c√°ch t√¨m trang cu·ªëi c√πng"""
        url = f"{self.base_url}/van-ban-trang-1.htm"
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            pagination = soup.find('ul', class_='pagination')
            if not pagination:
                print("Kh√¥ng t√¨m th·∫•y pagination, th·ª≠ t√¨m theo c√°ch kh√°c...")
                return self._find_last_page_by_testing()

            max_page = 1
            links = pagination.find_all('a')

            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                match = re.search(r'trang-(\d+)\.htm', href)
                if match:
                    page_num = int(match.group(1))
                    max_page = max(max_page, page_num)

                if text.isdigit():
                    page_num = int(text)
                    max_page = max(max_page, page_num)

            next_btn = pagination.find('a', string='¬ª')
            if next_btn and max_page > 0:
                print(f"T√¨m th·∫•y {max_page} trang trong pagination, ƒëang ki·ªÉm tra th√™m...")
                actual_max = self._find_last_page_by_testing(start_page=max_page)
                max_page = max(max_page, actual_max)

            return max_page

        except Exception as e:
            print(f"L·ªói khi l·∫•y s·ªë trang: {e}")
            return self._find_last_page_by_testing()

    def _find_last_page_by_testing(self, start_page=1):
        """T√¨m trang cu·ªëi b·∫±ng c√°ch test t·ª´ng trang"""
        print("ƒêang t√¨m trang cu·ªëi b·∫±ng binary search...")

        current = start_page
        step = 10

        while current <= 500:
            url = f"{self.base_url}/van-ban-trang-{current}.htm"
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    table = soup.find('table')
                    if table and len(table.find_all('tr')) > 1:
                        print(f"  Trang {current} t·ªìn t·∫°i ‚úì")
                        current += step
                    else:
                        print(f"  Trang {current} kh√¥ng c√≥ d·ªØ li·ªáu ‚úó")
                        break
                else:
                    print(f"  Trang {current} kh√¥ng t·ªìn t·∫°i ‚úó")
                    break
            except:
                print(f"  L·ªói khi ki·ªÉm tra trang {current}")
                break

        if current > start_page + step:
            low = current - step
            high = current - 1

            print(f"Binary search t·ª´ trang {low} ƒë·∫øn {high}...")

            while low <= high:
                mid = (low + high) // 2
                url = f"{self.base_url}/van-ban-trang-{mid}.htm"

                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        table = soup.find('table')
                        if table and len(table.find_all('tr')) > 1:
                            low = mid + 1
                        else:
                            high = mid - 1
                    else:
                        high = mid - 1
                except:
                    high = mid - 1

            return high

        return max(1, current - step)

    def crawl_page(self, page_num=1):
        """Crawl m·ªôt trang vƒÉn b·∫£n"""
        url = f"{self.base_url}/van-ban-trang-{page_num}.htm?Field=0&Agency=0&Type=0&keyword="

        print(f"ƒêang crawl trang {page_num}: {url}")

        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            table = soup.find('table')
            if not table:
                print(f"Kh√¥ng t√¨m th·∫•y b·∫£ng d·ªØ li·ªáu ·ªü trang {page_num}")
                return []

            documents = []
            rows = table.find_all('tr')[1:]

            if not rows:
                print(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")
                return []

            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 6:
                    doc = {
                        'so_ky_hieu': cols[0].get_text(strip=True),
                        'loai_van_ban': cols[1].get_text(strip=True),
                        'linh_vuc': cols[2].get_text(strip=True),
                        'trich_yeu': cols[3].get_text(strip=True),
                        'ngay_ban_hanh': cols[4].get_text(strip=True),
                        'download_link': None
                    }

                    download_td = cols[5]
                    download_link = download_td.find('a')
                    if download_link and download_link.get('href'):
                        doc['download_link'] = urljoin(self.base_url, download_link['href'])

                    documents.append(doc)

            print(f"‚úì Trang {page_num}: T√¨m th·∫•y {len(documents)} vƒÉn b·∫£n")
            return documents

        except Exception as e:
            print(f"‚úó L·ªói khi crawl trang {page_num}: {e}")
            return []

    def get_file_extension(self, url, content_type=None):
        """X√°c ƒë·ªãnh ƒë√∫ng extension c·ªßa file"""
        url_ext = os.path.splitext(url.lower())[1]
        if url_ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']:
            return url_ext

        if content_type:
            content_type = content_type.lower()
            if 'pdf' in content_type:
                return '.pdf'
            elif 'msword' in content_type or 'document' in content_type:
                return '.doc'
            elif 'wordprocessingml' in content_type:
                return '.docx'
            elif 'ms-excel' in content_type or 'spreadsheet' in content_type:
                if 'sheet' in content_type:
                    return '.xlsx'
                return '.xls'
            elif 'zip' in content_type:
                return '.zip'

        return '.pdf'

    def download_file(self, url, base_filename):
        """T·∫£i xu·ªëng file vƒÉn b·∫£n v·ªõi extension ƒë√∫ng"""
        try:
            response = self.session.head(url, timeout=10, allow_redirects=True)
            content_type = response.headers.get('Content-Type', '')

            ext = self.get_file_extension(url, content_type)

            safe_name = re.sub(r'[^\w\-.]', '_', base_filename)
            safe_name = os.path.splitext(safe_name)[0]
            filename = f"{safe_name}{ext}"

            print(f"ƒêang t·∫£i: {filename}")

            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()

            filepath = os.path.join(self.output_dir, filename)

            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            file_size = os.path.getsize(filepath)
            print(f"‚úì ƒê√£ t·∫£i: {filename} ({file_size / 1024:.1f} KB)")
            return True, filename, filepath

        except Exception as e:
            print(f"‚úó L·ªói khi t·∫£i {base_filename}: {e}")
            return False, None, None

    def process_document_api(self, file_path):
        """G·ªçi API process-document ƒë·ªÉ chuy·ªÉn file th√†nh markdown"""
        try:
            print(f"   üìÑ ƒêang x·ª≠ l√Ω document: {os.path.basename(file_path)}")

            with open(file_path, 'rb') as f:
                files = {'file': (os.path.basename(file_path), f)}
                response = requests.post(
                    f"http://localhost:8000/api/v1/process-document",
                    files=files,
                    timeout=60
                )

            if response.status_code == 200:
                result = response.json()
                print(f"   ‚úì Process document th√†nh c√¥ng")
                return result.get('markdown_content'), None
            else:
                print(f"   ‚úó Process document th·∫•t b·∫°i: {response.text}")
                return None, f"API error: {response.status_code}"

        except Exception as e:
            print(f"   ‚úó L·ªói khi g·ªçi process-document API: {e}")
            return None, str(e)

    def embed_markdown_api(self, markdown_content, document_id):
        """G·ªçi API embed-markdown ƒë·ªÉ t·∫°o embeddings v√† l∆∞u v√†o vector DB"""
        try:
            print(f"   üîó ƒêang t·∫°o embeddings cho document: {document_id}")

            payload = {
                "markdown_content": markdown_content,
                "document_id": document_id,
                "chunk_mode": "sentence"
            }

            response = requests.post(
                f"http://localhost:8000/api/v1/embed-markdown",
                json=payload,
                headers={'Content-Type': 'application/json'},
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                print(f"   ‚úì Embedding th√†nh c√¥ng: {result.get('stored_count')} chunks")
                return True, result
            else:
                print(f"   ‚úó Embedding th·∫•t b·∫°i: {response.text}")
                return False, f"API error: {response.status_code}"

        except Exception as e:
            print(f"   ‚úó L·ªói khi g·ªçi embed-markdown API: {e}")
            return False, str(e)

    def delete_document_embeddings(self, document_id):
        """
        X√≥a t·∫•t c·∫£ embeddings c·ªßa m·ªôt document_id

        Args:
            document_id: ID c·ªßa document c·∫ßn x√≥a

        Returns:
            tuple: (success: bool, message: str)
        """
        try:
            print(f"   üóëÔ∏è  ƒêang x√≥a embeddings cho document: {document_id}")

            response = requests.delete(
                f"http://localhost:8000/api/v1/document/delete/{document_id}",
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                print(f"   ‚úì X√≥a th√†nh c√¥ng: {result.get('message')}")
                return True, result.get('message', 'Document deleted successfully')
            else:
                error_msg = f"API error: {response.status_code} - {response.text}"
                print(f"   ‚úó X√≥a th·∫•t b·∫°i: {error_msg}")
                return False, error_msg

        except Exception as e:
            error_msg = f"L·ªói khi g·ªçi delete API: {str(e)}"
            print(f"   ‚úó {error_msg}")
            return False, error_msg

    def delete_embeddings_from_folder(self, folder_path=None):
        """
        X√≥a t·∫•t c·∫£ embeddings d·ª±a tr√™n c√°c file PDF/DOC/DOCX trong th∆∞ m·ª•c

        Args:
            folder_path: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file (m·∫∑c ƒë·ªãnh: output_dir)

        Returns:
            dict: Th·ªëng k√™ k·∫øt qu·∫£ x√≥a
        """
        if folder_path is None:
            folder_path = self.output_dir

        if not os.path.exists(folder_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {folder_path}")
            return {
                "success": False,
                "error": "Folder not found",
                "total": 0,
                "deleted": 0,
                "failed": 0
            }

        try:
            print("=" * 60)
            print("B·∫ÆT ƒê·∫¶U X√ìA EMBEDDINGS T·ª™ TH·ª¶ M·ª§C FILE")
            print("=" * 60)

            # T√¨m t·∫•t c·∫£ file PDF, DOC, DOCX, XLS, XLSX
            supported_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx']
            all_files = []

            for ext in supported_extensions:
                files = [f for f in os.listdir(folder_path)
                         if f.lower().endswith(ext)]
                all_files.extend(files)

            total_files = len(all_files)

            if total_files == 0:
                print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file n√†o trong th∆∞ m·ª•c: {folder_path}")
                return {
                    "success": True,
                    "total": 0,
                    "deleted": 0,
                    "failed": 0,
                    "message": "No files to delete"
                }

            print(f"\nüìä T√¨m th·∫•y {total_files} files trong th∆∞ m·ª•c")
            print(f"üìÅ Th∆∞ m·ª•c: {folder_path}")
            print("-" * 60)

            deleted_count = 0
            failed_count = 0
            results = []

            for idx, filename in enumerate(all_files, 1):
                # T·∫°o document_id t·ª´ t√™n file (b·ªè extension)
                filename_without_ext = os.path.splitext(filename)[0]
                # Sanitize gi·ªëng nh∆∞ khi embed
                document_id = re.sub(r'[^\w\-_.]', '_', filename_without_ext)

                print(f"\n[{idx}/{total_files}] X√≥a: {filename}")
                print(f"   Document ID: {document_id}")

                success, message = self.delete_document_embeddings(document_id)

                if success:
                    deleted_count += 1
                    results.append({
                        "filename": filename,
                        "document_id": document_id,
                        "status": "deleted",
                        "message": message
                    })
                else:
                    failed_count += 1
                    results.append({
                        "filename": filename,
                        "document_id": document_id,
                        "status": "failed",
                        "error": message
                    })

                # Delay nh·ªè gi·ªØa c√°c request
                if idx < total_files:
                    time.sleep(0.3)

            print("\n" + "=" * 60)
            print("K·∫æT QU·∫¢ X√ìA EMBEDDINGS")
            print("=" * 60)
            print(f"‚úì T·ªïng s·ªë files: {total_files}")
            print(f"‚úì X√≥a th√†nh c√¥ng: {deleted_count}")
            print(f"‚úó X√≥a th·∫•t b·∫°i: {failed_count}")
            print(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {(deleted_count / total_files * 100):.1f}%")

            return {
                "success": True,
                "total": total_files,
                "deleted": deleted_count,
                "failed": failed_count,
                "success_rate": round(deleted_count / total_files * 100, 1) if total_files > 0 else 0,
                "results": results
            }

        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a embeddings: {e}")
            return {
                "success": False,
                "error": str(e),
                "total": 0,
                "deleted": 0,
                "failed": 0
            }

        """
        X√≥a t·∫•t c·∫£ embeddings c·ªßa c√°c documents ƒë√£ ƒë∆∞·ª£c crawl t·ª´ CSV

        Args:
            csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV (m·∫∑c ƒë·ªãnh: output_dir/danh_sach_van_ban.csv)

        Returns:
            dict: Th·ªëng k√™ k·∫øt qu·∫£ x√≥a
        """
        if csv_file_path is None:
            csv_file_path = os.path.join(self.output_dir, 'danh_sach_van_ban.csv')

        if not os.path.exists(csv_file_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file CSV: {csv_file_path}")
            return {
                "success": False,
                "error": "CSV file not found",
                "total": 0,
                "deleted": 0,
                "failed": 0
            }

        try:
            print("=" * 60)
            print("B·∫ÆT ƒê·∫¶U X√ìA T·∫§T C·∫¢ EMBEDDINGS")
            print("=" * 60)

            # ƒê·ªçc CSV
            df = pd.read_csv(csv_file_path, encoding='utf-8-sig')

            # L·ªçc c√°c documents ƒë√£ c√≥ embedding
            if 'embedding_status' in df.columns:
                embedded_docs = df[df['embedding_status'] == 'success'].copy()
            else:
                # N·∫øu kh√¥ng c√≥ c·ªôt embedding_status, l·∫•y t·∫•t c·∫£ documents c√≥ so_ky_hieu
                embedded_docs = df[df['so_ky_hieu'].notna()].copy()

            total_docs = len(embedded_docs)

            if total_docs == 0:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y document n√†o c·∫ßn x√≥a")
                return {
                    "success": True,
                    "total": 0,
                    "deleted": 0,
                    "failed": 0,
                    "message": "No documents to delete"
                }

            print(f"\nüìä T√¨m th·∫•y {total_docs} documents c·∫ßn x√≥a embeddings")
            print("-" * 60)

            deleted_count = 0
            failed_count = 0
            results = []

            for idx, row in embedded_docs.iterrows():
                so_ky_hieu = row['so_ky_hieu']

                # T·∫°o document_id t·ª´ s·ªë k√Ω hi·ªáu (gi·ªëng nh∆∞ khi embed)
                document_id = re.sub(r'[^\w\-_.]', '_', str(so_ky_hieu))

                print(f"\n[{idx + 1}/{total_docs}] X√≥a: {so_ky_hieu}")

                success, message = self.delete_document_embeddings(document_id)

                if success:
                    deleted_count += 1
                    results.append({
                        "document_id": document_id,
                        "so_ky_hieu": so_ky_hieu,
                        "status": "deleted",
                        "message": message
                    })
                else:
                    failed_count += 1
                    results.append({
                        "document_id": document_id,
                        "so_ky_hieu": so_ky_hieu,
                        "status": "failed",
                        "error": message
                    })

                # Delay nh·ªè gi·ªØa c√°c request
                if idx < total_docs - 1:
                    time.sleep(0.5)

            print("\n" + "=" * 60)
            print("K·∫æT QU·∫¢ X√ìA EMBEDDINGS")
            print("=" * 60)
            print(f"‚úì T·ªïng s·ªë documents: {total_docs}")
            print(f"‚úì X√≥a th√†nh c√¥ng: {deleted_count}")
            print(f"‚úó X√≥a th·∫•t b·∫°i: {failed_count}")
            print(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {(deleted_count / total_docs * 100):.1f}%")

            return {
                "success": True,
                "total": total_docs,
                "deleted": deleted_count,
                "failed": failed_count,
                "success_rate": round(deleted_count / total_docs * 100, 1) if total_docs > 0 else 0,
                "results": results
            }

        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a embeddings: {e}")
            return {
                "success": False,
                "error": str(e),
                "total": 0,
                "deleted": 0,
                "failed": 0
            }

    def crawl_and_embed(self, max_pages=None, download_files=True, auto_embed=True, delay=1):
        """
        Crawl vƒÉn b·∫£n v√† t·ª± ƒë·ªông embedding v√†o vector DB

        Args:
            max_pages: S·ªë trang t·ªëi ƒëa c·∫ßn crawl (None = t·∫•t c·∫£)
            download_files: C√≥ t·∫£i file kh√¥ng
            auto_embed: C√≥ t·ª± ƒë·ªông embedding kh√¥ng
            delay: Th·ªùi gian delay gi·ªØa c√°c request (gi√¢y)
        """
        print("=" * 60)
        print("B·∫ÆT ƒê·∫¶U CRAWL & EMBEDDING DX.GOV.VN")
        print("=" * 60)

        # L·∫•y t·ªïng s·ªë trang
        print("\n[1/5] ƒêang x√°c ƒë·ªãnh t·ªïng s·ªë trang...")
        total_pages = self.get_total_pages()
        print(f"‚úì T·ªïng s·ªë trang t√¨m th·∫•y: {total_pages}")

        if max_pages:
            total_pages = min(total_pages, max_pages)
            print(f"‚úì Gi·ªõi h·∫°n crawl: {total_pages} trang")

        all_documents = []

        # Crawl t·ª´ng trang
        print(f"\n[2/5] B·∫Øt ƒë·∫ßu crawl {total_pages} trang...")
        print("-" * 60)

        for page in range(1, total_pages + 1):
            documents = self.crawl_page(page)
            all_documents.extend(documents)

            if page % 10 == 0 or page == total_pages:
                print(f"   Progress: {page}/{total_pages} trang ({len(all_documents)} vƒÉn b·∫£n)")

            if page < total_pages:
                time.sleep(delay)

        print("-" * 60)
        print(f"‚úì Ho√†n th√†nh crawl: {len(all_documents)} vƒÉn b·∫£n t·ª´ {total_pages} trang")

        # L∆∞u d·ªØ li·ªáu
        print(f"\n[3/5] ƒêang l∆∞u d·ªØ li·ªáu...")
        df = pd.DataFrame(all_documents)
        csv_path = os.path.join(self.output_dir, 'danh_sach_van_ban.csv')
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"‚úì ƒê√£ l∆∞u danh s√°ch v√†o: {csv_path}")

        # T·∫£i xu·ªëng v√† embedding file
        if download_files:
            print(f"\n[4/5] B·∫Øt ƒë·∫ßu t·∫£i file...")
            print("-" * 60)

            downloaded = 0
            failed = 0
            skipped = 0
            embedded = 0
            embed_failed = 0
            total_with_link = len([d for d in all_documents if d['download_link']])

            for idx, doc in enumerate(all_documents, 1):
                if doc['download_link']:
                    print(f"\nüì• [{idx}/{len(all_documents)}] {doc['so_ky_hieu']}")

                    # T·∫£i file
                    success, saved_filename, filepath = self.download_file(
                        doc['download_link'],
                        doc['so_ky_hieu']
                    )

                    if success:
                        downloaded += 1
                        doc['saved_file'] = saved_filename
                        doc['file_path'] = filepath

                        # Auto embedding n·∫øu ƒë∆∞·ª£c b·∫≠t
                        if auto_embed and filepath:
                            # T·∫°o document_id t·ª´ s·ªë k√Ω hi·ªáu
                            document_id = re.sub(r'[^\w\-_.]', '_', doc['so_ky_hieu'])

                            # Process document
                            markdown_content, error = self.process_document_api(filepath)

                            if markdown_content:
                                # Embed markdown
                                embed_success, embed_result = self.embed_markdown_api(
                                    markdown_content,
                                    document_id
                                )

                                if embed_success:
                                    embedded += 1
                                    doc['embedding_status'] = 'success'
                                    doc['embeddings_count'] = embed_result.get('stored_count', 0)
                                    doc['document_id'] = document_id  # L∆∞u document_id
                                else:
                                    embed_failed += 1
                                    doc['embedding_status'] = 'failed'
                                    doc['embedding_error'] = str(embed_result)
                            else:
                                embed_failed += 1
                                doc['embedding_status'] = 'process_failed'
                                doc['embedding_error'] = str(error)

                        # Progress
                        if downloaded % 5 == 0:
                            print(f"\nüìä Progress: {downloaded}/{total_with_link} files downloaded")
                            if auto_embed:
                                print(f"   üîó Embedded: {embedded}/{downloaded}")
                            time.sleep(delay)
                    else:
                        failed += 1
                        doc['saved_file'] = None
                        doc['embedding_status'] = 'download_failed'
                else:
                    skipped += 1
                    doc['saved_file'] = None
                    doc['embedding_status'] = 'no_link'

            print("-" * 60)
            print(f"\n=== TH·ªêNG K√ä T·∫¢I FILE ===")
            print(f"‚úì T·∫£i th√†nh c√¥ng: {downloaded}/{total_with_link}")
            print(f"‚úó T·∫£i th·∫•t b·∫°i: {failed}")
            print(f"‚äò Kh√¥ng c√≥ link: {skipped}")

            if auto_embed:
                print(f"\n=== TH·ªêNG K√ä EMBEDDING ===")
                print(f"‚úì Embedding th√†nh c√¥ng: {embedded}/{downloaded}")
                print(f"‚úó Embedding th·∫•t b·∫°i: {embed_failed}")
                print(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {(embedded / downloaded * 100):.1f}%" if downloaded > 0 else "0%")

            # C·∫≠p nh·∫≠t l·∫°i CSV v·ªõi th√¥ng tin file ƒë√£ l∆∞u
            df = pd.DataFrame(all_documents)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"\n‚úì ƒê√£ c·∫≠p nh·∫≠t: {csv_path}")

        print("\n" + "=" * 60)
        print("HO√ÄN TH√ÄNH!")
        print("=" * 60)

        return df


# S·ª≠ d·ª•ng
if __name__ == "__main__":
    # Kh·ªüi t·∫°o crawler v·ªõi API URL m·ªõi
    crawler = DXGovCrawlerWithEmbedding(
        output_dir="van_ban_downloads"
    )

    # ===== MENU L·ª∞A CH·ªåN =====
    print("\n" + "=" * 60)
    print("ü§ñ DX.GOV.VN CRAWLER & EMBEDDING TOOL")
    print("=" * 60)
    print("Ch·ªçn ch·ª©c nƒÉng:")
    print("1. Crawl v√† Embed vƒÉn b·∫£n (KHUY·∫æN NGH·ªä)")
    print("2. X√≥a t·∫•t c·∫£ embeddings t·ª´ th∆∞ m·ª•c file")
    print("3. X√≥a t·∫•t c·∫£ embeddings t·ª´ CSV")
    print("4. X√≥a m·ªôt document c·ª• th·ªÉ")
    print("0. Tho√°t")
    print("=" * 60)

    choice = input("\nNh·∫≠p l·ª±a ch·ªçn c·ªßa b·∫°n (0-4): ").strip()

    if choice == "1":
        # CRAWL V√Ä EMBED
        print("\n" + "=" * 60)
        print("üöÄ CRAWL V√Ä EMBED VƒÇN B·∫¢N")
        print("=" * 60)

        # H·ªèi s·ªë trang
        max_pages_input = input("\nS·ªë trang mu·ªën crawl (Enter = t·∫•t c·∫£, ho·∫∑c nh·∫≠p s·ªë): ").strip()
        max_pages = None
        if max_pages_input and max_pages_input.isdigit():
            max_pages = int(max_pages_input)
            print(f"‚úì S·∫Ω crawl {max_pages} trang")
        else:
            print("‚úì S·∫Ω crawl T·∫§T C·∫¢ trang (c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian)")

        # X√°c nh·∫≠n
        confirm = input("\nB·∫Øt ƒë·∫ßu crawl? (y/n): ").strip().lower()

        if confirm == 'y' or confirm == 'yes':
            print("\nüöÄ B·∫Øt ƒë·∫ßu crawl v√† embedding...")
            df = crawler.crawl_and_embed(
                max_pages=max_pages,
                download_files=True,
                auto_embed=True,
                delay=2
            )

            print("\n‚úÖ HO√ÄN TH√ÄNH!")
            print(f"üìä T·ªïng s·ªë vƒÉn b·∫£n: {len(df)}")
            if 'embedding_status' in df.columns:
                print(f"‚úì Embedded th√†nh c√¥ng: {(df['embedding_status'] == 'success').sum()}")
        else:
            print("\n‚ùå ƒê√£ h·ªßy!")

    elif choice == "2":
        # X√ìA T·ª™ TH·ª¶ M·ª§C FILE
        print("\n‚ö†Ô∏è  C·∫¢NH B√ÅO: B·∫°n s·∫Øp x√≥a T·∫§T C·∫¢ embeddings t·ª´ c√°c file ƒë√£ download!")
        print("=" * 60)

        folder_path = crawler.output_dir
        if not os.path.exists(folder_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {folder_path}")
        else:
            # ƒê·∫øm s·ªë file
            supported_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx']
            file_count = sum(len([f for f in os.listdir(folder_path)
                                  if f.lower().endswith(ext)])
                             for ext in supported_extensions)

            print(f"üìÅ Th∆∞ m·ª•c: {folder_path}")
            print(f"üìä T√¨m th·∫•y: {file_count} files")

            if file_count == 0:
                print("\n‚ö†Ô∏è  Kh√¥ng c√≥ file n√†o ƒë·ªÉ x√≥a!")
            else:
                confirm = input("\nX√°c nh·∫≠n x√≥a embeddings c·ªßa t·∫•t c·∫£ file? (y/n): ").strip().lower()

                if confirm == 'y' or confirm == 'yes':
                    print("\nüöÄ B·∫Øt ƒë·∫ßu x√≥a embeddings...")
                    result = crawler.delete_embeddings_from_folder()

                    print("\n" + "=" * 60)
                    print("üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG")
                    print("=" * 60)
                    print(f"   T·ªïng files: {result['total']}")
                    print(f"   ‚úì X√≥a th√†nh c√¥ng: {result['deleted']}")
                    print(f"   ‚úó X√≥a th·∫•t b·∫°i: {result['failed']}")
                    if result.get('success_rate'):
                        print(f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {result['success_rate']}%")
                else:
                    print("\n‚ùå ƒê√£ h·ªßy!")

    elif choice == "3":
        # X√ìA T·ª™ CSV
        print("\n‚ö†Ô∏è  C·∫¢NH B√ÅO: B·∫°n s·∫Øp x√≥a T·∫§T C·∫¢ embeddings t·ª´ CSV ƒë√£ crawl!")
        print("=" * 60)

        csv_path = os.path.join(crawler.output_dir, 'danh_sach_van_ban.csv')
        if not os.path.exists(csv_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file CSV: {csv_path}")
        else:
            confirm = input("\nX√°c nh·∫≠n x√≥a? (y/n): ").strip().lower()

            if confirm == 'y' or confirm == 'yes':
                print("\nüöÄ B·∫Øt ƒë·∫ßu x√≥a embeddings...")
                result = crawler.delete_all_embeddings_from_csv()

                print("\n" + "=" * 60)
                print("üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG")
                print("=" * 60)
                print(f"   T·ªïng documents: {result['total']}")
                print(f"   ‚úì X√≥a th√†nh c√¥ng: {result['deleted']}")
                print(f"   ‚úó X√≥a th·∫•t b·∫°i: {result['failed']}")
                if result.get('success_rate'):
                    print(f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {result['success_rate']}%")
            else:
                print("\n‚ùå ƒê√£ h·ªßy!")

    elif choice == "4":
        # X√ìA M·ªòT DOCUMENT C·ª§ TH·ªÇ
        print("\nüóëÔ∏è  X√ìA M·ªòT DOCUMENT C·ª§ TH·ªÇ")
        print("=" * 60)

        document_id = input("Nh·∫≠p document_id c·∫ßn x√≥a: ").strip()

        if document_id:
            confirm = input(f"\nX√°c nh·∫≠n x√≥a document '{document_id}'? (y/n): ").strip().lower()

            if confirm == 'y' or confirm == 'yes':
                success, message = crawler.delete_document_embeddings(document_id)
                if success:
                    print(f"\n‚úÖ {message}")
                else:
                    print(f"\n‚ùå {message}")
            else:
                print("\n‚ùå ƒê√£ h·ªßy!")
        else:
            print("\n‚ùå Document ID kh√¥ng h·ª£p l·ªá!")

    elif choice == "0":
        print("\nüëã T·∫°m bi·ªát!")

    else:
        print("\n‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")

    # ===== S·ª¨ D·ª§NG TR·ª∞C TI·∫æP KH√îNG QUA MENU =====

    # C√°ch 1: Crawl v√† embed tr·ª±c ti·∫øp
    # df = crawler.crawl_and_embed(
    #     max_pages=5,  # S·ªë trang mu·ªën crawl (None = t·∫•t c·∫£)
    #     download_files=True,
    #     auto_embed=True,
    #     delay=2
    # )

    # C√°ch 2: X√≥a t·ª´ th∆∞ m·ª•c
    # result = crawler.delete_embeddings_from_folder()

    # C√°ch 3: X√≥a t·ª´ CSV
    # result = crawler.delete_all_embeddings_from_csv()

    # C√°ch 4: X√≥a document c·ª• th·ªÉ
    # success, message = crawler.delete_document_embeddings("123_2024_QD-UBND")
