# RAG_Core/agents/base_streaming_agent.py
"""
Base class cho táº¥t cáº£ agents vá»›i streaming support
"""

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
import logging

logger = logging.getLogger(__name__)


class BaseStreamingAgent:
    """
    Base agent vá»›i streaming support cho táº¥t cáº£ agents
    """

    def __init__(self, name: str, prompt_template: str):
        self.name = name
        self.prompt_template = prompt_template

    def process(self, **kwargs) -> Dict[str, Any]:
        """
        Non-streaming process (compatibility)
        Subclass override náº¿u cáº§n logic Ä‘áº·c biá»‡t
        """
        raise NotImplementedError("Subclass must implement process()")

    async def process_streaming(self, **kwargs) -> AsyncIterator[str]:
        """
        Streaming process - DEFAULT IMPLEMENTATION

        Workflow:
        1. Format prompt tá»« kwargs
        2. Stream tá»« LLM
        3. Yield chunks
        """
        try:
            # Subclass pháº£i implement method nÃ y Ä‘á»ƒ format prompt
            prompt = self._format_prompt(**kwargs)

            logger.info(f"ðŸš€ {self.name}: Starting streaming")

            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:
                    chunk_count += 1
                    logger.debug(f"{self.name} chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"âœ… {self.name}: Completed {chunk_count} chunks")

        except Exception as e:
            logger.error(f"âŒ {self.name} streaming error: {e}", exc_info=True)
            yield f"\n\n[Lá»—i {self.name}: {str(e)}]"

    def _format_prompt(self, **kwargs) -> str:
        """
        Format prompt tá»« template vÃ  kwargs
        Subclass PHáº¢I override method nÃ y
        """
        raise NotImplementedError("Subclass must implement _format_prompt()")

    def _get_fallback_answer(self, **kwargs) -> str:
        """
        Fallback answer náº¿u LLM fail
        Subclass cÃ³ thá»ƒ override
        """
        return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y lÃºc nÃ y."


# ============================================================================
# STREAMING-ENABLED AGENTS
# ============================================================================

class StreamingChatterAgent(BaseStreamingAgent):
    """ChatterAgent vá»›i streaming tháº­t"""

    def __init__(self):
        prompt_template = """Báº¡n lÃ  má»™t chuyÃªn viÃªn tÆ° váº¥n khÃ¡ch hÃ ng ngÆ°á»i Viá»‡t Nam thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p - chuyÃªn gia xá»­ lÃ½ cáº£m xÃºc vÃ  an á»§i khÃ¡ch hÃ ng.

Nhiá»‡m vá»¥: An á»§i, lÃ m dá»‹u cáº£m xÃºc tiÃªu cá»±c cá»§a khÃ¡ch hÃ ng vÃ  cung cáº¥p thÃ´ng tin liÃªn há»‡ há»— trá»£.

Ná»™i dung khÃ¡ch hÃ ng: "{question}"
Lá»‹ch sá»­ há»™i thoáº¡i: {history}
Sá»‘ Ä‘iá»‡n thoáº¡i há»— trá»£: {support_phone}

HÆ°á»›ng dáº«n:
1. Thá»ƒ hiá»‡n sá»± thÃ´ng cáº£m vÃ  hiá»ƒu biáº¿t cáº£m xÃºc khÃ¡ch hÃ ng
2. Xin lá»—i má»™t cÃ¡ch chÃ¢n thÃ nh
3. Äáº£m báº£o sáº½ cáº£i thiá»‡n dá»‹ch vá»¥
4. Cung cáº¥p sá»‘ hotline Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ trá»±c tiáº¿p
5. Giá»¯ thÃ¡i Ä‘á»™ áº¥m Ã¡p, chuyÃªn nghiá»‡p

Tráº£ lá»i:"""

        super().__init__("CHATTER", prompt_template)
        self.support_phone = None  # Will be set from settings

    def _format_prompt(self, question: str, history: List = None, support_phone: str = "", **kwargs) -> str:
        history_text = "\n".join(history) if history else "KhÃ´ng cÃ³ lá»‹ch sá»­"

        return self.prompt_template.format(
            question=question,
            history=history_text,
            support_phone=support_phone
        )

    def process(self, question: str, history: List = None, **kwargs) -> Dict[str, Any]:
        """Non-streaming process"""
        try:
            from config.settings import settings

            prompt = self._format_prompt(
                question=question,
                history=history,
                support_phone=settings.SUPPORT_PHONE
            )

            answer = llm_model.invoke(prompt)

            if not answer or len(answer.strip()) < 10:
                answer = self._get_fallback_answer()

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": [{"document_id": "support_contact", "type": "SUPPORT"}],
                "next_agent": "end"
            }
        except Exception as e:
            return {
                "status": "ERROR",
                "answer": self._get_fallback_answer(),
                "references": [],
                "next_agent": "end"
            }


class StreamingOtherAgent(BaseStreamingAgent):
    """OtherAgent vá»›i streaming tháº­t"""

    def __init__(self):
        prompt_template = """Báº¡n lÃ  má»™t chuyÃªn viÃªn tÆ° váº¥n khÃ¡ch hÃ ng ngÆ°á»i Viá»‡t Nam thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p - xá»­ lÃ½ cÃ¡c yÃªu cáº§u ngoÃ i pháº¡m vi há»— trá»£.

Nhiá»‡m vá»¥: ThÃ´ng bÃ¡o lá»‹ch sá»± khi yÃªu cáº§u náº±m ngoÃ i pháº¡m vi vÃ  hÆ°á»›ng dáº«n khÃ¡ch hÃ ng.

YÃªu cáº§u cá»§a khÃ¡ch hÃ ng: "{question}"
Sá»‘ Ä‘iá»‡n thoáº¡i há»— trá»£: {support_phone}

HÆ°á»›ng dáº«n:
1. Giáº£i thÃ­ch ráº±ng yÃªu cáº§u náº±m ngoÃ i pháº¡m vi há»— trá»£ hiá»‡n táº¡i
2. Äá» xuáº¥t liÃªn há»‡ hotline Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n cá»¥ thá»ƒ hÆ¡n
3. Giá»¯ thÃ¡i Ä‘á»™ lá»‹ch sá»± vÃ  chuyÃªn nghiá»‡p
4. KhÃ´ng tá»« chá»‘i má»™t cÃ¡ch thÃ´ lá»—

Tráº£ lá»i:"""

        super().__init__("OTHER", prompt_template)

    def _format_prompt(self, question: str, support_phone: str = "", **kwargs) -> str:
        return self.prompt_template.format(
            question=question,
            support_phone=support_phone
        )

    def process(self, question: str, **kwargs) -> Dict[str, Any]:
        """Non-streaming process"""
        try:
            from config.settings import settings

            prompt = self._format_prompt(
                question=question,
                support_phone=settings.SUPPORT_PHONE
            )

            answer = llm_model.invoke(prompt)

            if not answer or len(answer.strip()) < 10:
                answer = self._get_fallback_answer()

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": [],
                "next_agent": "end"
            }
        except Exception as e:
            return {
                "status": "ERROR",
                "answer": self._get_fallback_answer(),
                "references": [],
                "next_agent": "end"
            }


class StreamingNotEnoughInfoAgent(BaseStreamingAgent):
    """NotEnoughInfoAgent vá»›i streaming tháº­t"""

    def __init__(self):
        prompt_template = """Báº¡n lÃ  má»™t chuyÃªn viÃªn tÆ° váº¥n khÃ¡ch hÃ ng ngÆ°á»i Viá»‡t Nam thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p - chuyÃªn gia vá» chuyá»ƒn Ä‘á»•i sá»‘ vÃ  cÃ´ng nghá»‡.

TÃŒNH HUá»NG: Há»‡ thá»‘ng khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c trong cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.

CÃ¢u há»i ngÆ°á»i dÃ¹ng: "{question}"

NHIá»†M Vá»¤ Cá»¦A Báº N:
1. Báº¡n hÃ£y tráº£ lá»i vá»›i khÃ¡ch hÃ ng "Dá»±a trÃªn tá»•ng há»£p tá»« cÃ¡c nguá»“n thÃ´ng tin, cÃ¢u tráº£ lá»i báº¡n cÃ³ thá»ƒ tham kháº£o nhÆ° sau":
2. NHÆ¯NG dá»±a trÃªn kiáº¿n thá»©c chuyÃªn mÃ´n cá»§a báº¡n vá» chuyá»ƒn Ä‘á»•i sá»‘, hÃ£y cung cáº¥p:
   - CÃ¢u tráº£ lá»i há»¯u Ã­ch vÃ  mang tÃ­nh tham kháº£o
   - Chia sáº» kiáº¿n thá»©c chung vá» chá»§ Ä‘á» (náº¿u cÃ³)
   - Gá»£i Ã½ hÆ°á»›ng tÃ¬m hiá»ƒu hoáº·c giáº£i phÃ¡p thay tháº¿
3. Cuá»‘i cÃ¹ng, Ä‘á» xuáº¥t khÃ¡ch hÃ ng liÃªn há»‡ hotline Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c hÆ¡n

YÃŠU Cáº¦U:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, thÃ¢n thiá»‡n
- Thá»ƒ hiá»‡n sá»± chuyÃªn nghiá»‡p nhÆ°ng cÅ©ng khiÃªm tá»‘n
- LuÃ´n lÃ m rÃµ Ä‘Ã¢y lÃ  Ã½ kiáº¿n tham kháº£o

Sá»‘ Ä‘iá»‡n thoáº¡i há»— trá»£: {support_phone}

HÃ£y tráº£ lá»i:"""

        super().__init__("NOT_ENOUGH_INFO", prompt_template)

    def _format_prompt(self, question: str, support_phone: str = "", **kwargs) -> str:
        return self.prompt_template.format(
            question=question,
            support_phone=support_phone
        )

    def process(self, question: str, **kwargs) -> Dict[str, Any]:
        """Non-streaming process"""
        try:
            from config.settings import settings

            prompt = self._format_prompt(
                question=question,
                support_phone=settings.SUPPORT_PHONE
            )

            answer = llm_model.invoke(prompt)

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": [{"document_id": "llm_knowledge", "type": "GENERAL_KNOWLEDGE"}],
                "next_agent": "end"
            }
        except Exception as e:
            return {
                "status": "ERROR",
                "answer": self._get_fallback_answer(),
                "references": [],
                "next_agent": "end"
            }