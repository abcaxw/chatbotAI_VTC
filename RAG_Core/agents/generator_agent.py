# RAG_Core/agents/generator_agent.py - FIXED STREAMING VERSION

from typing import Dict, Any, List, AsyncIterator
from models.llm_model import llm_model
import logging

logger = logging.getLogger(__name__)


class GeneratorAgent:
    def __init__(self):
        self.name = "GENERATOR"

        # Standard prompt (unchanged)
        self.standard_prompt = """B·∫°n l√† m·ªôt chuy√™n vi√™n t∆∞ v·∫•n kh√°ch h√†ng ng∆∞·ªùi Vi·ªát Nam th√¢n thi·ªán v√† chuy√™n nghi·ªáp.

C√¢u h·ªèi c·ªßa kh√°ch h√†ng: "{question}"

Th√¥ng tin tham kh·∫£o t·ª´ t√†i li·ªáu:
{documents}

L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y:
{history}

Y√™u c·∫ßu tr·∫£ l·ªùi:
- Tr·∫£ l·ªùi b·∫±ng gi·ªçng vƒÉn t·ª± nhi√™n nh∆∞ ng∆∞·ªùi Vi·ªát Nam n√≥i chuy·ªán
- Tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, ng·∫Øn g·ªçn s√∫c t√≠ch
- D·ª±a v√†o th√¥ng tin t√†i li·ªáu nh∆∞ng di·ªÖn ƒë·∫°t theo c√°ch hi·ªÉu c·ªßa b·∫°n
- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ng·∫Øn ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£ n·∫øu c·∫ßn

H√£y tr·∫£ l·ªùi nh∆∞ ƒëang n√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi kh√°ch h√†ng:"""

        # Follow-up prompt (unchanged)
        self.followup_prompt = """B·∫°n l√† m·ªôt chuy√™n vi√™n t∆∞ v·∫•n kh√°ch h√†ng ng∆∞·ªùi Vi·ªát Nam th√¢n thi·ªán v√† chuy√™n nghi·ªáp.

üîç NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN:
{context_summary}

üìù L·ªäCH S·ª¨ G·∫¶N NH·∫§T:
{recent_history}

‚ùì C√ÇU H·ªéI FOLLOW-UP C·ª¶A KH√ÅCH H√ÄNG: "{question}"

üìö TH√îNG TIN T√ÄI LI·ªÜU LI√äN QUAN:
{documents}

‚ö†Ô∏è Y√äU C·∫¶U ƒê·∫∂C BI·ªÜT cho follow-up question:
1. Nh·∫≠n bi·∫øt r·∫±ng kh√°ch h√†ng ƒëang h·ªèi ti·∫øp v·ªÅ ch·ªß ƒë·ªÅ ƒë√£ th·∫£o lu·∫≠n
2. Tham chi·∫øu ƒë·∫øn th√¥ng tin ƒë√£ cung c·∫•p tr∆∞·ªõc ƒë√≥ m·ªôt c√°ch t·ª± nhi√™n
3. Tr·∫£ l·ªùi c·ª• th·ªÉ v√†o ph·∫ßn m√† kh√°ch h√†ng mu·ªën bi·∫øt th√™m
4. KH√îNG l·∫∑p l·∫°i to√†n b·ªô th√¥ng tin ƒë√£ n√≥i, ch·ªâ t·∫≠p trung v√†o ph·∫ßn ƒë∆∞·ª£c h·ªèi

üìã Y√äU C·∫¶U CHUNG:
- Tr·∫£ l·ªùi b·∫±ng gi·ªçng vƒÉn t·ª± nhi√™n nh∆∞ ng∆∞·ªùi Vi·ªát Nam n√≥i chuy·ªán
- Ng·∫Øn g·ªçn, s√∫c t√≠ch, ƒë√∫ng tr·ªçng t√¢m
- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£ n·∫øu c·∫ßn

H√£y tr·∫£ l·ªùi:"""

    def _deduplicate_references(self, references: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Lo·∫°i b·ªè references tr√πng l·∫∑p"""
        if not references:
            return []

        seen_doc_ids = set()
        unique_references = []

        for ref in references:
            doc_id = ref.get('document_id')
            if doc_id and doc_id not in seen_doc_ids:
                seen_doc_ids.add(doc_id)
                unique_references.append(ref)

        return unique_references

    def _format_documents(self, documents: List[Dict[str, Any]]) -> str:
        """Format documents th√†nh text"""
        if not documents:
            return "Kh√¥ng c√≥ t√†i li·ªáu tham kh·∫£o"

        doc_lines = []
        for i, doc in enumerate(documents[:5], 1):
            description = doc.get('description', '')
            score = doc.get('similarity_score', 0)
            doc_lines.append(f"[T√†i li·ªáu {i}] (ƒê·ªô li√™n quan: {score:.2f})\n{description}")

        return "\n\n".join(doc_lines)

    def _format_history(self, history: List, max_turns: int = 2) -> str:
        """Format l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        if not history:
            return "Kh√¥ng c√≥ l·ªãch s·ª≠"

        # Normalize history
        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append({
                    "role": msg.get("role", ""),
                    "content": msg.get("content", "")
                })
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        # L·∫•y N turn g·∫ßn nh·∫•t
        recent_history = normalized_history[-(max_turns * 2):] if len(
            normalized_history) > max_turns * 2 else normalized_history

        history_lines = []
        for msg in recent_history:
            role = "üë§ Kh√°ch h√†ng" if msg.get("role") == "user" else "ü§ñ Tr·ª£ l√Ω"
            content = msg.get("content", "")
            if content:
                history_lines.append(f"{role}: {content}")

        return "\n".join(history_lines) if history_lines else "Kh√¥ng c√≥ l·ªãch s·ª≠"

    def _extract_context_summary(self, history: List) -> str:
        """Tr√≠ch xu·∫•t context summary"""
        if not history or len(history) < 2:
            return "ƒê√¢y l√† c√¢u h·ªèi ƒë·∫ßu ti√™n"

        # Normalize history
        normalized_history = []
        for msg in history:
            if isinstance(msg, dict):
                normalized_history.append(msg)
            else:
                normalized_history.append({
                    "role": getattr(msg, "role", ""),
                    "content": getattr(msg, "content", "")
                })

        # L·∫•y c√¢u h·ªèi tr∆∞·ªõc
        for i in range(len(normalized_history) - 1, -1, -1):
            if normalized_history[i].get("role") == "user":
                prev_question = normalized_history[i].get("content", "")

                for j in range(i + 1, len(normalized_history)):
                    if normalized_history[j].get("role") == "assistant":
                        prev_answer = normalized_history[j].get("content", "")
                        return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}\nƒê√£ tr·∫£ l·ªùi: {prev_answer[:200]}..."

                return f"Ch·ªß ƒë·ªÅ ƒëang th·∫£o lu·∫≠n: {prev_question}"

        return "ƒêang trong cu·ªôc tr√≤ chuy·ªán"

    def process(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> Dict[str, Any]:
        """Non-streaming generation (original)"""
        try:
            if not documents:
                return {
                    "status": "ERROR",
                    "answer": "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi",
                    "references": [],
                    "next_agent": "end"
                }

            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Ch·ªçn prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.followup_prompt.format(
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text
                )
            else:
                prompt = self.standard_prompt.format(
                    question=question,
                    history=history_text,
                    documents=doc_text
                )

            # Generate answer (non-streaming)
            answer = llm_model.invoke(prompt)

            if not answer or len(answer.strip()) < 10:
                answer = "T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin li√™n quan nh∆∞ng g·∫∑p kh√≥ khƒÉn trong vi·ªác t·∫°o c√¢u tr·∫£ l·ªùi."

            unique_references = self._deduplicate_references(references or [])

            return {
                "status": "SUCCESS",
                "answer": answer,
                "references": unique_references,
                "next_agent": "end"
            }

        except Exception as e:
            logger.error(f"Error in generator agent: {e}", exc_info=True)
            return {
                "status": "ERROR",
                "answer": f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}",
                "references": [],
                "next_agent": "end"
            }

    async def process_streaming(
            self,
            question: str,
            documents: List[Dict[str, Any]],
            references: List[Dict[str, Any]] = None,
            history: List[Dict[str, str]] = None,
            is_followup: bool = False,
            context_summary: str = "",
            **kwargs
    ) -> AsyncIterator[str]:
        """
        FIXED: Streaming generation with proper async/await

        Returns async generator that yields text chunks
        """
        try:
            logger.info(f"üöÄ Generator: Starting streaming for: {question[:50]}...")

            if not documents:
                yield "Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."
                return

            # Format inputs
            doc_text = self._format_documents(documents)
            history_text = self._format_history(history or [], max_turns=2)

            # Choose prompt
            if is_followup:
                if not context_summary:
                    context_summary = self._extract_context_summary(history or [])

                prompt = self.followup_prompt.format(
                    question=question,
                    context_summary=context_summary,
                    recent_history=history_text,
                    documents=doc_text
                )
            else:
                prompt = self.standard_prompt.format(
                    question=question,
                    history=history_text,
                    documents=doc_text
                )

            logger.info(f"üìù Generator: Prompt prepared, length={len(prompt)}")

            # CRITICAL: Stream from LLM
            chunk_count = 0
            async for chunk in llm_model.astream(prompt):
                if chunk:  # Only yield non-empty chunks
                    chunk_count += 1
                    logger.debug(f"Generator yielding chunk #{chunk_count}: {chunk[:30]}...")
                    yield chunk

            logger.info(f"‚úÖ Generator: Completed streaming {chunk_count} chunks")

        except Exception as e:
            logger.error(f"‚ùå Generator streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói: {str(e)}]"